.[[RR PAPERS]].  [[LRN.]]
  [[RR DEFINITIONS]],
  [[RR Blog]], 


mamba block


### 2025-04-22  Code Buff

[[codebuff.pdf]] 


### 2025-04-22  Non-parametric Feature Impact and Importance

[[2006.04750v1.pdf]]


### 2025-02-20  Verify step by step

https://arxiv.org/pdf/2305.20050



### 2024-12-18  DETRs Beat YOLOs on real-time object detection

https://arxiv.org/abs/2304.08069



Outcome-supervised Reward Models (ORMs) -- trained using final result of the model’s chain-of-thought,
Process-supervised Reward Models (PRMs) --  receive feedback for each step in the chain of thought.


Our main contributions are as follows:
1. We show that process supervision can train much more reliable reward
models than outcome supervision. We use our state-of-the-art PRM to
solve 78.2% of problems from a representative subset of the MATH test
set.
2. We show that a large reward model can reliably approximate human supervision for smaller reward models, and that it can be used to efficiently
conduct large-scale data collection ablations.
3. We show that active learning leads to a 2.6× improvement in the data
efficiency of process supervision.
4. We release our full



### !2024-05-13 - [The Platonic Representation Hypothesis](https://arxiv.org/abs/2405.07987)

- Representational Convergence -- The idea that structures in model weights are converging.

Observations
- Splicing lower levels from one transformer onto another with an affine transform works well
- "We might say: all strong models are alike, each weak model is weak in its own way."
- representations are aligned with the structure of human brains
- COLOR - we verified that convergence occurs in real data


Why
- MANY TASKS PIN IT DOWN - The need to solve many tasks at once leads to a few possible solutions.
- SIMPLICITY PINS IT DOWN - 
- [[Contrastive Learning]] - are all gunning for the same optimality point (assuming a BIJECTIVE (lossless) sensing of the world)
- TO - "a statistical model of the underlying reality"
- TO = "a unified model that is proficient across various domains and modalities, grounded in the statistical properties of the underlying world. "

### !2024-06-01 - [Attention is all you need](https://arxiv.org/pdf/1706.03762) 
[Medium Article](https://towardsdatascience.com/attention-is-all-you-need-discovering-the-transformer-paper-73e5ff5e0634):
[[Transformers]] 


### !2013-09-07 - [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781) 

