
- (See Torch [[Transformer Module]])
- [[DL Notation]], 
- [[VAEs]], 
- [[Activation Functions]],   [[Auto Encoders]],  [[VAEs]],





#### FAMILIES:
- GPT-like (also called _auto-regressive_ Transformer models)
- BERT-like (also called _auto-encoding_ Transformer models)
- [[Seq2Seq]] - BART/T5-like (also called _sequence-to-sequence_ Transformer models)

### Key Ideas
- Before transformer's attention was used with recurrent networks
- [[Embedding]] 
- [[Positional Encoding]]
- [[Self Attention]]
- [[Attention]], [[Multi-headed Attention Block]], 
- [[ReLU]] activation
- [[Regularization]] 
- Adjust parameters in parallel (unlike RNNs)
- Positional Embeddings

