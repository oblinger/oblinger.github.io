
Adam;; is a gradient descent alternative that combines momentum in exponentially weighted average of gradients and Root Mean Square Propagation

- https://www.geeksforgeeks.org/adam-optimizer/?ref=ml_lbp


