---
layout: cayman
title: ASI Outcomes
description: A framework for reasoning about and mitigating risks associated with ASI
permalink: /ASIO/ASI_Outcomes/
---

<img src="../../assets/images/WorkInProgress.png" height="150px;" />

*I plan on refining/adding content before sharing it more widely.*

<br>

**TL; DR.**  *Splitting arguments about plausible outcomes for ASI into isolated claims allows us to identify sub-areas having broad consensus and provide Lego-like blocks one can use to construct the range of most plausible outcomes.*

<br>

Artificial Super Intelligence (ASI) is arguably the most transformative advancement humanity will ever make.  Predictions about outcomes from researchers in the field range from visions of utopia to utter destruction.
There are too many unknowns to reason our way to a consensus conclusion; still, when talking with AI researchers, I find many areas of broad agreement.

This suggests we identify and isolate areas of relative consensus, then combine subsets of these to envision a couple of plausible futures. This approach allows us to agree on a collection of plausible futures and highlight the specific sub-claims that separate the divergent thinking on this pressing existential question.

Below is a starter hierarchy of about a dozen plausible claims. The aim is to frame as many "Obvious" claims as possible to serve as the building blocks for more contentious conclusions. Ultimately, I would like an open wiki-style framework where researchers might post other claims and some mechanism to record consensus and dissent for each posited claim. For now, the focus is simply on capturing a handful of claims that tie together to form interesting conclusions. Feedback is most welcome at <a href="mailto:public@oblinger.us">public@oblinger.us</a>.

<br>

<br>

|                                                                    |                                                                                                                                                                                                      |
| ------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Abbreviation**                                                   | **CULMINATING CLAIMS**                                                                                                                                                                               |
| **[Can't Stop](/ASIO/Cant_Stop)**                                  | Claim: Humanity cannot stop its creation of AGI.                                                                                                                                                     |
| **[Can't Control](/ASIO/Cant_Control)**                            | Claim: Humanity cannot maintain indefinite control over the ASI systems it creates (even if alignment is solved).                                                                                    |
| **[Our Best Hope](Our_Best_Hope.md)**                              | Claim: Our best hope is to shape our progressive loss of control in ways least likely to result in catastrophic harm.                                                                                |
|                                                                    |                                                                                                                                                                                                      |
|                                                                    | **WONKISH SUB-CLAIMS**                                                                                                                                                                               |
| **[AGI Implies ASI](/ASIO/AGI_implies_ASI)**                       | Claim: Achieving AGI will likely afford us a specific kind of ASI at nearly the same time.                                                                                                           |
| **[Corp AI](/ASIO/Corp_AI)**                                       | Claim: Nations, Corporations, and Individuals are, to a first order of approximation, profit/power maximization mechanisms.                                                                          |
| **[Loop Closing](/ASIO/Loop_Closing)**                             | Claim: Once loop is closed over an entire job function, performance on that job function will jump discontinuously in cost and productivity.                                                         |
| **[Agentic Decompositionality](/ASIO/Agentic_Decompositionality)** | Claim: While not required of all AGI systems, knowledge, ability, and agency can be encoded as separable concerns.                                                                                   |
| **[Alignment Won't Work](/ASIO/Alignment_Wont_Work)**              | Claim: Because of a layered combination of reasons, humanity will fail to align the AIs we build.                                                                                                    |
| **[Natural Selection](/ASIO/Natural_Selection)**                   | Claim: People, organizations, or AI-Agents embedded within a process of natural selection cannot escape or halt that process of natural selection.                                                   |
| **[Machine Centric Coms](/ASIO/Machine_Centric_Coms)**             | Claim: The fraction of machine-interpretable information will grow to eclipse human-interpretable communications. This will be true both between and within intelligent agents.                      |
|                                                                    |                                                                                                                                                                                                      |
|                                                                    | **HALF BAKED CLAIMS**                                                                                                                                                                                |
| **[Apex Predator](/ASIO/Apex_Predator)**                           | Claim: We are in the final stages of a planet-wide transition from one apex predator to another.                                                                                                     |
| **[AGI Will Come](/ASIO/AGI_Will_Come)**                           |                                                                                                                                                                                                      |
| **[Nature of AI Progress](/ASIO/Nature_of_AI_Progress)**           | Claim: There are specific aspects of successful AI systems that we can use to make informed predictions about the nature of future AI systems and our probable progress in developing these systems. |
| **[Civilizing Tendency](/ASIO/Civilizing_Tendency)**               | Claim: All Corp-AI agents will tend to have the following instrumental purposes as a core part of their agenda.                                                                                      |
| **[No Fast Takeoff](/ASIO/No_Fast_Takeoff)**                       | Claim: A unilateral fast take-off scenario might be less likely while risks from a slow takeoff might be greater than often considered.                                                              |






