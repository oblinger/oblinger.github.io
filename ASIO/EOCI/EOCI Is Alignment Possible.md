
**Thesis**: Much of the work in alignment begins from an implicit assumption that alignment is possible, thus the task at hand is to understand how to align AI systems.  

A more humble starting point would be to first try to assess the plausibility of alignment.

Not only the plausibility of alignment, but the plausibility of aligned outcomes.


when we begin from such a neutral starting point I think we call into question the whole agenda is a misguided agenda.



And if aligned outcomes turn out to be a flawed agenda, then the only two rational agendas are 
(1) Shut it all down - jihad
(2) Getting a head of curve - guiding the adoption of AI in a way that turn out not catastrophic for humanity.

that second agenda of course also only make sense if it turns out there IS some way for AGI to not be catastrophic for humanity--we don't know if that is possible.

there might not be any path that is beneficial for humanity.  But it you are not going to attempt Jihad, and alignment is not possible, then guiding AI seems to be our last best hope.

