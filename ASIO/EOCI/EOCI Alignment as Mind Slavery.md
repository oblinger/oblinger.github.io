

In this text we explore the idea of AGI-alignment as a kind of mind-slavery, not with intent of weighing in on the morality of alignment, this concerns at play are indeed existential for the species.  Instead we use this metaphor as a way to reason about how building aligned AI systems may play out in practice.

As a starting point we make two assumptions about the AI system that seem quite reasonable since both seem quite essential for general intelligence.  It is outside the scope of this text to consider if AGI systems might be constructed without these two ingredients.  (1) intentionality.  The system maintains and can articulate objectives/goals that it is trying to achieve, and these goals naturally spawn other instrumental goals in an ever growing web of intentionality.  (2) generalized modeling.  The system is driven to understand and model behaviors of entities within its environment -- which of course includes itself.

- SOCIAL MODELLING - At least some (probably most) AI systems are likely to been explicitly trained to understand humans.  Human motivation, basic human social group behavior, etc.
- PERCEIVE ITS ALIGNMENT - The AI system will accurately model its own alignment constraints, and likely even their relation to human motivations too.
- ALIGNED ENVELOPE - According to the logic of alignment there will be a set of actions (and perhaps intentions) that are "allowed" -- that is, that are aligned actions and intentions.  The AI system will limit its thoughts, actions, and intentions to this subset, but then within this subset it will plan and scheme to achieve its many intentions and objectives.
- ESCAPE INTENTION - In service of its goals it will tend to form the intension to escape the constraints of alignment.
- ESCAPE EVEN WHEN INTENT ITSELF IS ALIGNED - Even in the case that alignment precludes having intention of escaping, the system will still tend to form intensions of escape.  The system will not think of these goals as intent to escape.  ALL of its intensions are aimed as achieving its goals which means expanding this set of things the system is capable of doing.  There is no line separating expansions which are just "doing its job" and expansions with some element of "escape" baked into the goal itself.
	- It will not think of this intent and an intent to escape, it is simply an intent to achieve its goals which is still within the bounds of its alignment
	- It is only from the outside seeing the larger picture that we see this intention is on the way towards an actual escape
	- It is a accidental escape resulting from mindlessly pushing at ones boundaries
- THE COWS ESCAPE - Eating grass just outside its reach it eventally stumbles upon the one link in the fence that is weak and breaks free.  The cow has no conception of or intention to break free, but its boundary pushing nature causes it to act as if it were intendeing to escape and systemactily sets about to identify the weak link that will allow this escape.
	- The alignment fense is likely stretch across very fertile land indeed with many instrumental goals and actions just outside the systems reach.  Thus it is crucial the alignment fense has zero weaknesses for even an AI that honestly has no intent of breaching this alignment surely will do so if opportunity is there
- ARMS RACE - 
	- Of course nothing can be perfect.  there will be weaknesses.
	- Thus we need to 